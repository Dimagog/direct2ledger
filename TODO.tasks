Now:
✔ Download txns @done(18-10-02 15:46)
✔ Parse downloaded txns @done(18-10-02 15:46)
☐ Fix `removeDuplicates`
  ✔ Better yet remove duplicates while processing downloaded txns @done(18-10-02 15:46)
  ☐ Print duplicate txns
✔ Put FITID into generated journal @done(18-10-02 16:10)
✔ Parse FITID from journal @done(18-10-02 16:25)
☐ Control startDate or how many days back to download

Soon:
☐ Proper implementation for reading OFX from file
☐ Change `[]txn` to `[]*txn`
☐ Storing amounts as `float` is imprecise
☐ Automatically detect how many days to download
☐ Only store TIDs of txns within downloaded date range 9with some slack)

Later:
  ☐ Download txns in parallel

Archive:
  ✔ Stream-processing of journal @done(18-10-01 10:25)
  ✔ Aggregate journal txns into bag of hashes and set of TIDs @done(18-10-01 10:25)
  ✔ Calculate txn hash on-demand @done(18-10-01 10:25)
  ✔ Get rid of parser.data @done(18-09-30 18:38) @project(Internal Improvements)
  ✔ Extract accounts using `ledger accounts` @done(18-09-30 18:38) @project(Internal Improvements)
  ✘ Or better yet add accounts on the fly while processing ledger file @cancelled(18-09-30 18:38) @project(Internal Improvements)
  ✔ Do stream processing of converted journal CSV instead of `Cmd.Output` @done(18-09-30 18:38) @project(Internal Improvements)
  ✔ Instead of `os.Create` for output file use `os.Open` with proper flags (no truncate) @done(18-09-30 10:17) @project(Internal Improvements)
